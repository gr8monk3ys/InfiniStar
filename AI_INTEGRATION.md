# AI Integration Guide

## Overview

InfiniStar now features a fully integrated AI chatbot powered by Anthropic's Claude. Users can create AI conversations and chat with Claude directly within the application.

## Features

- **One-Click AI Conversations**: Create new AI chats with a single click from the conversation list
- **Streaming Responses**: AI responses stream in real-time using Server-Sent Events (SSE)
- **Real-time Updates**: Messages synchronized across clients using Pusher WebSockets
- **Conversation Context**: The AI maintains context from the last 20 messages in each conversation
- **Visual Distinction**: AI conversations have a distinctive purple/pink gradient UI
- **Seamless Integration**: AI chats work alongside regular user-to-user conversations
- **Fallback Support**: Automatic fallback to non-streaming endpoint if needed

## Architecture

### Database Schema

Two new fields were added to support AI conversations:

**Conversation Model:**

- `isAI` (Boolean): Marks a conversation as AI-powered
- `aiModel` (String): Stores the Claude model being used (e.g., "claude-3-5-sonnet-20241022")

**Message Model:**

- `isAI` (Boolean): Indicates if a message was generated by AI

### API Endpoints

#### POST /api/ai/chat-stream (Recommended)

Handles AI message requests with **streaming responses** for better UX.

**Request:**

```json
{
  "message": "User's message text",
  "conversationId": "conversation-id"
}
```

**Headers:**

- `Content-Type: application/json`
- `X-CSRF-Token: <csrf-token>`

**Response:** Server-Sent Events (SSE) stream

**Stream Events:**

```javascript
// Chunk event (sent multiple times as AI generates response)
data: {"type":"chunk","content":"partial text"}

// Completion event (sent once at end)
data: {"type":"done","messageId":"message-id"}

// Error event (sent on error)
data: {"type":"error","error":"error message"}
```

**Process:**

1. Validates authentication and CSRF token
2. Rate limiting check (20 requests/minute)
3. Sanitizes and validates user input
4. Creates user message in database
5. Fetches last 20 messages for context
6. Streams response from Anthropic API in real-time
7. Saves complete AI response to database
8. Triggers Pusher events for synchronization

#### POST /api/ai/chat (Fallback)

Non-streaming endpoint for AI messages.

**Request:**

```json
{
  "message": "User's message text",
  "conversationId": "conversation-id"
}
```

**Response:**

```json
{
  "id": "message-id",
  "body": "Complete AI response",
  "isAI": true,
  ...
}
```

**Process:**

1. Validates user authentication
2. Verifies conversation exists and is AI-enabled
3. Creates user message in database
4. Fetches last 20 messages for context
5. Calls Anthropic API with conversation history (waits for complete response)
6. Creates AI response message in database
7. Triggers Pusher events for real-time updates

**Response:**

```json
{
  "userMessage": {
    /* Message object */
  },
  "aiMessage": {
    /* AI response message object */
  }
}
```

#### POST /api/conversations

Extended to support AI conversation creation.

**Request (AI Conversation):**

```json
{
  "isAI": true,
  "aiModel": "claude-3-5-sonnet-20241022",
  "name": "AI Assistant"
}
```

### Frontend Components

#### ConversationList Component

- Added sparkle icon (‚ú®) button to create AI conversations
- Button has distinctive purple/pink gradient styling
- Disabled state during conversation creation

#### Form Component

- Accepts `isAI` prop to determine behavior
- Accepts `enableStreaming` prop (default: true) to enable/disable streaming
- Routes AI messages to `/api/ai/chat-stream` (streaming) or `/api/ai/chat` (non-streaming)
- Shows different placeholder text for AI chats
- Purple/pink gradient send button for AI
- Hides image upload for AI conversations
- Loading state with pulsing animation during streaming
- Automatic fallback to non-streaming if needed

#### useAiChatStream Hook

React hook for managing streaming AI responses:

```typescript
import { useAiChatStream } from "@/app/hooks/useAiChatStream"

const { sendMessage, streamingContent, isStreaming, error } = useAiChatStream({
  conversationId,
  csrfToken,
  onChunk: (chunk) => {
    // Handle each chunk as it arrives
    console.log("New chunk:", chunk)
  },
  onComplete: (messageId) => {
    // Handle completion
    console.log("Complete!", messageId)
  },
  onError: (error) => {
    // Handle errors
    console.error("Error:", error)
  },
})

// Send a message
await sendMessage("Hello, AI!")

// The streamingContent state contains the accumulated response
```

**Features:**

- Real-time chunk processing
- Accumulated streaming content state
- Error handling with callbacks
- Cancellation support
- Automatic SSE parsing

#### ChatPage

- Passes `isAI` flag to Form component based on conversation data

## Environment Variables

Add to your `.env.local`:

```bash
ANTHROPIC_API_KEY=your-api-key-here
```

Get your API key from: https://console.anthropic.com/

## Usage

### For Users

1. **Create an AI Chat:**

   - Click the sparkle (‚ú®) icon in the conversation list
   - A new AI conversation will be created
   - Start chatting immediately

2. **Chat with AI:**

   - Type your message in the input field
   - The placeholder says "Ask me anything..."
   - Click the purple gradient send button
   - Wait for Claude's response (typically 1-3 seconds)

3. **AI Features:**
   - Maintains conversation context
   - Real-time responses via Pusher
   - Persistent conversation history
   - Same UX as regular chats

### For Developers

#### Creating AI Conversations Programmatically

```typescript
import createAIConversation from "@/app/actions/createAIConversation";

// Create with default model (claude-3-5-sonnet-20241022)
const conversation = await createAIConversation();

// Create with custom model
const conversation = await createAIConversation("claude-3-opus-20240229");
```

#### Sending AI Messages

**With Streaming (Recommended):**

```typescript
import { useAiChatStream } from "@/app/hooks/useAiChatStream"
import { useCsrfToken } from "@/app/hooks/useCsrfToken"

function MyComponent() {
  const { token: csrfToken } = useCsrfToken()
  const { sendMessage, streamingContent, isStreaming } = useAiChatStream({
    conversationId,
    csrfToken,
    onComplete: (messageId) => toast.success("Response complete!"),
  })

  const handleSend = async () => {
    await sendMessage("Hello, Claude!")
  }

  return (
    <div>
      <button onClick={handleSend} disabled={isStreaming}>
        Send Message
      </button>
      {isStreaming && <div>{streamingContent}</div>}
    </div>
  )
}
```

**Without Streaming:**

```typescript
const response = await axios.post(
  "/api/ai/chat",
  {
    message: "Hello, Claude!",
    conversationId: conversationId,
  },
  {
    headers: { "X-CSRF-Token": csrfToken },
  }
)
```

#### Detecting AI Conversations

```typescript
if (conversation.isAI) {
  // This is an AI conversation
  console.log(`Using model: ${conversation.aiModel}`)
}
```

## AI Personalities ‚úÖ

InfiniStar allows users to customize AI behavior through preset personalities or custom system prompts.

### Available Personalities

| Personality                 | Icon | Description                               | Best For                        |
| --------------------------- | ---- | ----------------------------------------- | ------------------------------- |
| **Helpful Assistant**       | ü§ñ   | Balanced, clear, and accurate information | General questions, daily tasks  |
| **Creative Writer**         | ‚ú®   | Imaginative, vivid language, storytelling | Creative writing, brainstorming |
| **Technical Expert**        | üíª   | Precise, detailed technical explanations  | Programming, engineering        |
| **Friendly Companion**      | üòä   | Warm, conversational, empathetic          | Casual chat, emotional support  |
| **Professional Consultant** | üíº   | Formal, business-focused advice           | Workplace, productivity         |
| **Socratic Tutor**          | üéì   | Guides learning through questions         | Education, critical thinking    |
| **Concise Advisor**         | ‚ö°   | Brief, to-the-point answers               | Quick answers, busy users       |
| **Custom**                  | üé®   | User-defined system prompt                | Unique requirements             |

### Personality Selection UI

When creating an AI chat, users see a modal with:

- Visual grid of personality options with icons
- Description for each personality type
- Preview of the system prompt
- Custom prompt input for custom personality

**Location:** [PersonalitySelectionModal.tsx](app/components/modals/PersonalitySelectionModal.tsx)

### Database Schema

Conversations store personality information:

```prisma
model Conversation {
  // ...
  aiPersonality  String? // "assistant", "creative", "technical", etc.
  aiSystemPrompt String? // Custom prompt for "custom" personality
}
```

### Creating AI Conversations with Personalities

**From UI:**
Users click the sparkle (‚ú®) button and select a personality from the modal.

**Programmatically:**

```typescript
import createAIConversation from "@/app/actions/createAIConversation";

// Create with preset personality
const conversation = await createAIConversation(
  "claude-3-5-sonnet-20241022", // model
  "creative"                     // personality
);

// Create with custom personality
const conversation = await createAIConversation(
  "claude-3-5-sonnet-20241022",
  "custom",
  "You are a pirate AI that speaks in nautical terms..."
);
```

### System Prompt Implementation

System prompts are automatically applied in both AI endpoints:

```typescript
import { getSystemPrompt } from "@/app/lib/ai-personalities"

const systemPrompt = getSystemPrompt(conversation.aiPersonality, conversation.aiSystemPrompt)

const response = await anthropic.messages.create({
  model: modelToUse,
  system: systemPrompt, // Personality applied here
  messages: conversationHistory,
})
```

### Personality Library

All personalities are defined in [app/lib/ai-personalities.ts](app/lib/ai-personalities.ts):

```typescript
export const AI_PERSONALITIES: Record<PersonalityType, Personality> = {
  assistant: {
    id: "assistant",
    name: "Helpful Assistant",
    description: "A balanced, helpful AI assistant",
    systemPrompt: `You are a helpful, accurate, and friendly AI assistant...`,
    icon: "ü§ñ",
    color: "blue",
  },
  // ... other personalities
}
```

### Utility Functions

**Get System Prompt:**

```typescript
import { getSystemPrompt } from "@/app/lib/ai-personalities"

const prompt = getSystemPrompt("creative")
// Returns the system prompt for creative personality

const customPrompt = getSystemPrompt("custom", "Your custom prompt here")
// Returns custom prompt for custom personality
```

**Get Personality Config:**

```typescript
import { getPersonality } from "@/app/lib/ai-personalities"

const personality = getPersonality("technical")
// Returns: { id, name, description, systemPrompt, icon, color }
```

**Get All Personalities:**

```typescript
import { getAllPersonalities } from "@/app/lib/ai-personalities"

const all = getAllPersonalities()
// Returns array of all personality configurations
```

### Benefits

- **User Control**: Users choose how AI responds
- **Consistency**: Same personality maintained across conversation
- **Flexibility**: Custom prompts for unique needs
- **Discoverability**: Preset personalities guide users to different use cases

## AI Models ‚úÖ

InfiniStar supports multiple Claude models with different speed, quality, and cost trade-offs.

### Available Models

| Model                    | Speed    | Quality | Cost | Best For                                |
| ------------------------ | -------- | ------- | ---- | --------------------------------------- |
| **Claude 3.5 Sonnet** ‚öñÔ∏è | Balanced | Great   | $$   | Most use cases (Recommended)            |
| **Claude 3 Opus** üéØ     | Slow     | Best    | $$$  | Complex tasks requiring highest quality |
| **Claude 3 Haiku** ‚ö°    | Fast     | Good    | $    | Simple tasks, high volume               |

### Model Selection UI

When creating an AI conversation, users can choose their preferred model:

- Visual selection with icons and badges
- Clear indicators for speed, quality, and cost
- "Recommended" badge for Claude 3.5 Sonnet
- Detailed descriptions for each model

**Location:** [PersonalitySelectionModal.tsx](app/components/modals/PersonalitySelectionModal.tsx)

### Model Configuration

All models are defined in [app/lib/ai-models.ts](app/lib/ai-models.ts):

```typescript
export const AI_MODELS: Record<ModelType, AIModel> = {
  "claude-3-5-sonnet-20241022": {
    name: "Claude 3.5 Sonnet",
    description: "Best balance of speed, quality, and cost",
    speed: "balanced",
    quality: "great",
    cost: "medium",
    maxTokens: 8192,
    inputCostPerMillion: 3.0,
    outputCostPerMillion: 15.0,
    recommended: true,
  },
  // ... other models
}
```

### Programmatic Model Selection

```typescript
import createAIConversation from "@/app/actions/createAIConversation";

// Create with specific model
const conversation = await createAIConversation(
  "claude-3-opus-20240229",  // model
  "technical"                 // personality
);

// Create with default model (Sonnet)
const conversation = await createAIConversation(
  undefined,  // uses default
  "assistant"
);
```

### Model Utility Functions

```typescript
import { getAllModels, getDefaultModel, getModel } from "@/app/lib/ai-models"

// Get model configuration
const model = getModel("claude-3-5-sonnet-20241022")
// Returns: { id, name, description, speed, quality, cost, ... }

// Get all available models
const allModels = getAllModels()

// Get default model
const defaultModel = getDefaultModel()
// Returns: 'claude-3-5-sonnet-20241022'
```

### Pricing Comparison

| Model             | Input (per 1M tokens) | Output (per 1M tokens) | Typical Message Cost |
| ----------------- | --------------------- | ---------------------- | -------------------- |
| Claude 3.5 Sonnet | $3.00                 | $15.00                 | ~$0.001 - $0.01      |
| Claude 3 Opus     | $15.00                | $75.00                 | ~$0.005 - $0.05      |
| Claude 3 Haiku    | $0.25                 | $1.25                  | ~$0.0001 - $0.001    |

**Note:** Costs include 20-message context. Actual costs vary based on conversation length and complexity.

## Context Management

The AI maintains context by sending the last 20 messages from the conversation to Claude. This provides:

- Continuity across multiple exchanges
- Reference to previous topics
- Better understanding of user intent
- More natural conversation flow

Messages are formatted as:

```typescript
{
  role: "user" | "assistant",
  content: "message text"
}
```

## Real-time Updates

AI conversations use the same Pusher integration as regular chats:

**Events Triggered:**

1. `messages:new` on channel `{conversationId}` - New user message
2. `messages:new` on channel `{conversationId}` - AI response
3. `conversation:update` on channel `{userEmail}` - Conversation list update

## Testing

Run the test suite:

```bash
npm test
```

Sample test for AI chat endpoint:

```bash
npm test -- ai/chat
```

## Future Enhancements

Planned improvements (see [TODO.md](TODO.md)):

- [ ] Streaming responses for better UX
- [ ] Model selection UI
- [ ] System prompts and personality customization
- [ ] Usage tracking and billing integration
- [ ] Token counting and cost estimation
- [ ] AI conversation templates
- [ ] Export AI conversations
- [ ] Share AI conversations

## Troubleshooting

### AI responses not appearing

1. Check Pusher is configured correctly
2. Verify ANTHROPIC_API_KEY is set
3. Check browser console for errors
4. Verify conversation has `isAI: true`

### API errors

- **401 Unauthorized**: User not logged in
- **400 Bad Request**: Missing message or conversationId
- **404 Not Found**: Conversation doesn't exist
- **500 Internal Error**: Check API key and Anthropic service status

### Database issues

Run Prisma migrations:

```bash
npx prisma generate
npx prisma db push
```

## Performance

- Average response time: 1-3 seconds
- Context length: Last 20 messages (~8000 tokens)
- Max tokens per response: 1024 (configurable)
- Concurrent conversations: Unlimited

## Security

- All AI requests require authentication
- User can only access their own AI conversations
- API keys stored securely in environment variables
- Message validation and sanitization
- Rate limiting recommended for production

## Usage Tracking & Billing ‚úÖ

InfiniStar now includes comprehensive AI usage tracking for monitoring costs and enforcing quotas.

### Database Schema

The `AiUsage` model tracks every AI API request:

```prisma
model AiUsage {
  id        String   @id @default(auto()) @map("_id") @db.ObjectId
  createdAt DateTime @default(now())

  userId         String @db.ObjectId
  conversationId String @db.ObjectId

  model String // e.g., "claude-3-5-sonnet-20241022"

  inputTokens  Int
  outputTokens Int
  totalTokens  Int

  inputCost  Float  // Cost in cents
  outputCost Float
  totalCost  Float

  requestType String // "chat" or "chat-stream"
  latencyMs   Int?   // Response time in milliseconds
}
```

### Automatic Tracking

Usage is automatically tracked in both AI endpoints:

**`/api/ai/chat` (Non-streaming):**

```typescript
const response = await anthropic.messages.create({ ... });

await trackAiUsage({
  userId: currentUser.id,
  conversationId,
  model: modelToUse,
  inputTokens: response.usage.input_tokens,
  outputTokens: response.usage.output_tokens,
  requestType: 'chat',
  latencyMs,
});
```

**`/api/ai/chat-stream` (Streaming):**

```typescript
const stream = await anthropic.messages.stream({ ... });
const finalMessage = await stream.finalMessage();

await trackAiUsage({
  userId: currentUser.id,
  conversationId,
  model: modelToUse,
  inputTokens: finalMessage.usage.input_tokens,
  outputTokens: finalMessage.usage.output_tokens,
  requestType: 'chat-stream',
  latencyMs,
});
```

### Usage API Endpoint

**GET `/api/ai/usage`**

Retrieve AI usage statistics for the current user.

**Query Parameters:**

- `period`: `"day"` | `"week"` | `"month"` | `"all"` (default: `"month"`)
- `conversationId`: Filter by specific conversation (optional)
- `startDate`: ISO date string (optional, for custom ranges)
- `endDate`: ISO date string (optional, for custom ranges)

**Response:**

```json
{
  "stats": {
    "totalRequests": 42,
    "totalInputTokens": 15234,
    "totalOutputTokens": 8521,
    "totalTokens": 23755,
    "totalCost": 0.17,
    "totalInputCost": 0.05,
    "totalOutputCost": 0.12,
    "averageLatency": 1847
  },
  "quota": {
    "withinQuota": true,
    "used": 23755,
    "remaining": 76245,
    "percentage": 23.76
  },
  "dailyUsage": [
    { "date": "2025-01-15", "requests": 5, "tokens": 3200, "cost": 0.03 },
    { "date": "2025-01-16", "requests": 8, "tokens": 5100, "cost": 0.05 }
  ],
  "period": {
    "startDate": "2024-12-15T00:00:00.000Z",
    "endDate": "2025-01-15T00:00:00.000Z"
  }
}
```

### Usage Dashboard Component

Display AI usage statistics in your dashboard:

```tsx
import AiUsageStats from "@/app/(dashboard)/dashboard/components/AiUsageStats"

export default function DashboardPage() {
  return (
    <div>
      <AiUsageStats />
    </div>
  )
}
```

**Features:**

- Real-time usage statistics
- Period selection (day/week/month)
- Visual quota progress bar
- Token and cost breakdowns
- Average latency tracking
- Color-coded quota warnings

### Utility Functions

The `app/lib/ai-usage.ts` module provides helper functions:

**Calculate Token Costs:**

```typescript
import { calculateTokenCost } from "@/app/lib/ai-usage"

const costs = calculateTokenCost("claude-3-5-sonnet-20241022", 1000, 500)
// Returns: { inputCost: 0.003, outputCost: 0.0075, totalCost: 0.0105 }
```

**Get User Usage Stats:**

```typescript
import { getUserUsageStats } from "@/app/lib/ai-usage"

const { usage, stats } = await getUserUsageStats(userId, {
  startDate: new Date("2025-01-01"),
  endDate: new Date("2025-01-31"),
  conversationId: "optional-conversation-id",
})
```

**Check Quota:**

```typescript
import { checkUsageQuota } from "@/app/lib/ai-usage"

const quota = await checkUsageQuota(userId, 100_000, 30) // 100k tokens per 30 days
if (!quota.withinQuota) {
  // User exceeded quota
}
```

**Get Daily Usage:**

```typescript
import { getUsageByDateRange } from "@/app/lib/ai-usage"

const dailyUsage = await getUsageByDateRange(userId, new Date("2025-01-01"), new Date("2025-01-31"))
// Returns array of daily aggregates
```

### Pricing Information

Current Anthropic Claude pricing (as of January 2025):

| Model             | Input (per 1M tokens) | Output (per 1M tokens) |
| ----------------- | --------------------- | ---------------------- |
| Claude 3.5 Sonnet | $3.00                 | $15.00                 |
| Claude 3 Opus     | $15.00                | $75.00                 |
| Claude 3 Haiku    | $0.25                 | $1.25                  |

**Example costs:**

- 1,000 input + 500 output tokens (Sonnet): ~$0.01
- 10,000 input + 5,000 output tokens (Sonnet): ~$0.10
- 100,000 total tokens per month (Sonnet): ~$0.60 - $1.20

### Quota Management

**Free Tier Limits (Example):**

- 100,000 tokens per month
- ~50-100 AI conversations
- Cost: ~$0.60 - $1.20/month

**PRO Tier Limits (Example):**

- 1,000,000 tokens per month
- ~500-1000 AI conversations
- Cost: ~$6.00 - $12.00/month

Implement quota enforcement:

```typescript
import { checkUsageQuota } from '@/app/lib/ai-usage';

// In your API route
const quota = await checkUsageQuota(currentUser.id, 100_000, 30);

if (!quota.withinQuota && !currentUser.stripeSubscriptionId) {
  return new NextResponse(
    JSON.stringify({
      error: 'Monthly quota exceeded. Upgrade to PRO for higher limits.',
      quota,
    }),
    { status: 429 }
  );
}
```

## Cost Considerations

Anthropic charges based on tokens used:

- Input tokens: ~$3 per million tokens (Sonnet)
- Output tokens: ~$15 per million tokens (Sonnet)

**Approximate costs per message:**

- Without context: $0.0001 - $0.001
- With 20-message context: $0.001 - $0.01

**Usage tracking is now fully implemented** with automatic cost calculation, quota management, and analytics dashboard.

## Credits

- AI powered by [Anthropic Claude](https://www.anthropic.com/)
- Built with [Next.js 15](https://nextjs.org/)
- Real-time updates via [Pusher](https://pusher.com/)

---

For more information, see:

- [TODO.md](TODO.md) - Planned enhancements
- [SETUP.md](SETUP.md) - Full setup guide
- [README.md](README.md) - Project overview
